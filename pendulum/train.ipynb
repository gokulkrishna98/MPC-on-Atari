{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e36d6813",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gymnasium as gym\n",
    "import random\n",
    "\n",
    "from collections import deque\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07a39c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_reward(theta, theta_dt, torque):\n",
    "    return -(theta**2 + 0.1 * theta_dt**2 + 0.001 * torque**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "857198e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PendulumRewardDataset(Dataset):\n",
    "    def __init__(self, size=10000):\n",
    "        self.size = size\n",
    "        self.data = []\n",
    "\n",
    "        for _ in range(size):\n",
    "            theta = np.random.uniform(-np.pi, np.pi)\n",
    "            theta_dt = np.random.uniform(-8.0, 8.0)\n",
    "            torque = np.random.uniform(-2.0, 2.0)\n",
    "            reward = true_reward(theta, theta_dt, torque)\n",
    "            self.data.append((theta, theta_dt, torque, reward))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        theta, theta_dt, torque, reward = self.data[idx]\n",
    "        x = torch.tensor([theta, theta_dt, torque], dtype=torch.float32)\n",
    "        y = torch.tensor([reward], dtype=torch.float32)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "220325e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PendulumRewardDataset(size=10000)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c381dbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(3, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8149541b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_reward_model(model, dataloader, epochs=10, lr=1e-3, device='cpu'):\n",
    "    model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            pred = model(x)\n",
    "            loss = criterion(pred, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_loss = running_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "215c091b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 10.9741\n",
      "Epoch 2, Loss: 1.9405\n",
      "Epoch 3, Loss: 0.3938\n",
      "Epoch 4, Loss: 0.1880\n",
      "Epoch 5, Loss: 0.1291\n",
      "Epoch 6, Loss: 0.0797\n",
      "Epoch 7, Loss: 0.0514\n",
      "Epoch 8, Loss: 0.0363\n",
      "Epoch 9, Loss: 0.0268\n",
      "Epoch 10, Loss: 0.0212\n"
     ]
    }
   ],
   "source": [
    "model = RewardModel()\n",
    "train_reward_model(model, dataloader, epochs=10, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf0de82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_prediction(model, theta, theta_dt, torque):\n",
    "    model.eval()\n",
    "    \n",
    "    # Input tensor\n",
    "    x = torch.tensor([[theta, theta_dt, torque]], dtype=torch.float32)\n",
    "    \n",
    "    # Model prediction\n",
    "    with torch.no_grad():\n",
    "        predicted_reward = model(x).item()\n",
    "    \n",
    "    # Ground truth\n",
    "    actual_reward = true_reward(theta, theta_dt, torque)\n",
    "    \n",
    "    print(f\"Input: theta={theta:.3f}, theta_dt={theta_dt:.3f}, torque={torque:.3f}\")\n",
    "    print(f\"Predicted Reward: {predicted_reward:.4f}\")\n",
    "    print(f\"True Reward:      {actual_reward:.4f}\")\n",
    "    print(f\"Error:            {abs(predicted_reward - actual_reward):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b5c4fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: theta=1.000, theta_dt=0.500, torque=-1.500\n",
      "Predicted Reward: -0.8990\n",
      "True Reward:      -1.0272\n",
      "Error:            0.1283\n"
     ]
    }
   ],
   "source": [
    "test_model_prediction(model, theta=1.0, theta_dt=0.5, torque=-1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73dddc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for DDPG\n",
    "\n",
    "GAMMA = 0.99\n",
    "TAU = 0.005\n",
    "ACTOR_LR = 1e-3\n",
    "CRITIC_LR = 1e-3\n",
    "MAX_EPISODES = 200\n",
    "MAX_STEPS = 200\n",
    "BUFFER_SIZE = 100000\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fc09e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3749a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward model\n",
    "def cost_fn(state, action, model=None):\n",
    "    theta = np.arctan2(state[1], state[0])\n",
    "    theta_dot = state[2]\n",
    "    torque = action[0]\n",
    "\n",
    "    # model.eval()\n",
    "    # x = torch.tensor([[theta, theta_dot, torque]], dtype=torch.float32)\n",
    "    # with torch.no_grad():\n",
    "    #     reward = model(x).item()\n",
    "    reward =  theta**2 + 0.1 * theta_dot**2 + 0.001 * (torque**2)\n",
    "    return reward\n",
    "\n",
    "# World model\n",
    "def true_dynamics(state, action, dt=0.05):\n",
    "    g = 10.0     # gravity\n",
    "    m = 1.0      # mass\n",
    "    l = 1.0      # length of pendulum\n",
    "    max_speed = 8.0\n",
    "    max_torque = 2.0\n",
    "\n",
    "    th = np.arctan2(state[1], state[0])  # angle θ\n",
    "    thdot = state[2]                     # angular velocity\n",
    "\n",
    "    u = np.clip(action, -max_torque, max_torque)[0]  # limit torque\n",
    "\n",
    "    # Apply the physics: θ̈ = dynamics equation\n",
    "    newthdot = thdot + (-3 * g / (2 * l) * np.sin(th + np.pi) + 3.0 / (m * l ** 2) * u) * dt\n",
    "    newthdot = np.clip(newthdot, -max_speed, max_speed)\n",
    "\n",
    "    newth = th + newthdot * dt\n",
    "\n",
    "    return np.array([np.cos(newth), np.sin(newth), newthdot])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d651181d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gokul/miniconda3/envs/ddrl_a4/lib/python3.8/site-packages/gymnasium/envs/classic_control/pendulum.py:178: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym.make(\"Pendulum-v1\", render_mode=\"rgb_array\")\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m seq:\n\u001b[1;32m     20\u001b[0m         total_cost \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m cost_fn(sim_state, a)\n\u001b[0;32m---> 21\u001b[0m         sim_state \u001b[38;5;241m=\u001b[39m \u001b[43mtrue_dynamics\u001b[49m\u001b[43m(\u001b[49m\u001b[43msim_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     costs\u001b[38;5;241m.\u001b[39mappend(total_cost)\n\u001b[1;32m     24\u001b[0m best_action \u001b[38;5;241m=\u001b[39m action_sequences[np\u001b[38;5;241m.\u001b[39margmin(costs)][\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[0;32mIn[7], line 27\u001b[0m, in \u001b[0;36mtrue_dynamics\u001b[0;34m(state, action, dt)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Apply the physics: θ̈ = dynamics equation\u001b[39;00m\n\u001b[1;32m     26\u001b[0m newthdot \u001b[38;5;241m=\u001b[39m thdot \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m \u001b[38;5;241m*\u001b[39m g \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m l) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msin(th \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mpi) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m3.0\u001b[39m \u001b[38;5;241m/\u001b[39m (m \u001b[38;5;241m*\u001b[39m l \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m*\u001b[39m u) \u001b[38;5;241m*\u001b[39m dt\n\u001b[0;32m---> 27\u001b[0m newthdot \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnewthdot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mmax_speed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_speed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m newth \u001b[38;5;241m=\u001b[39m th \u001b[38;5;241m+\u001b[39m newthdot \u001b[38;5;241m*\u001b[39m dt\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([np\u001b[38;5;241m.\u001b[39mcos(newth), np\u001b[38;5;241m.\u001b[39msin(newth), newthdot])\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mclip\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/ddrl_a4/lib/python3.8/site-packages/numpy/core/fromnumeric.py:2180\u001b[0m, in \u001b[0;36mclip\u001b[0;34m(a, a_min, a_max, out, **kwargs)\u001b[0m\n\u001b[1;32m   2111\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_clip_dispatcher)\n\u001b[1;32m   2112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclip\u001b[39m(a, a_min, a_max, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   2113\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2114\u001b[0m \u001b[38;5;124;03m    Clip (limit) the values in an array.\u001b[39;00m\n\u001b[1;32m   2115\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2178\u001b[0m \n\u001b[1;32m   2179\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2180\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclip\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma_min\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma_max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ddrl_a4/lib/python3.8/site-packages/numpy/core/fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[0;32m~/miniconda3/envs/ddrl_a4/lib/python3.8/site-packages/numpy/core/_methods.py:136\u001b[0m, in \u001b[0;36m_clip\u001b[0;34m(a, min, max, out, casting, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _clip_dep_is_byte_swapped(a) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _clip_dep_is_byte_swapped(out):\n\u001b[1;32m    135\u001b[0m     using_deprecated_nan \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43m_clip_dep_is_scalar_nan\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    137\u001b[0m         \u001b[38;5;28mmin\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    138\u001b[0m         using_deprecated_nan \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ddrl_a4/lib/python3.8/site-packages/numpy/core/_methods.py:95\u001b[0m, in \u001b[0;36m_clip_dep_is_scalar_nan\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_clip_dep_is_scalar_nan\u001b[39m(a):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;66;03m# guarded to protect circular imports\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfromnumeric\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ndim\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ndim(a) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Pendulum-v1\")\n",
    "state, _ = env.reset()\n",
    "horizon = 15\n",
    "num_samples = 100\n",
    "action_dim = env.action_space.shape[0]\n",
    "action_low = env.action_space.low\n",
    "action_high = env.action_space.high\n",
    "\n",
    "for t in range(200):\n",
    "    # Sample random action sequences\n",
    "    action_sequences = np.random.uniform(\n",
    "        low=action_low, high=action_high, size=(num_samples, horizon, action_dim)\n",
    "    )\n",
    "\n",
    "    costs = []\n",
    "    for seq in action_sequences:\n",
    "        sim_state = np.copy(state)\n",
    "        total_cost = 0\n",
    "        for a in seq:\n",
    "            total_cost += cost_fn(sim_state, a)\n",
    "            sim_state = true_dynamics(sim_state, a)\n",
    "        costs.append(total_cost)\n",
    "\n",
    "    best_action = action_sequences[np.argmin(costs)][0]\n",
    "    state, _, terminated, truncated, _ = env.step(best_action)\n",
    "    env.render()\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ddrl_a4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
